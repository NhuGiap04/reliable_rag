{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3932c434",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4994243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e5150b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd485be",
   "metadata": {},
   "source": [
    "**Load Multi-hop Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48df676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('VDT2025_Multihop_RAG/multihoprag_corpus.txt', 'r') as f:\n",
    "    # Read the entire file content\n",
    "    corpus = f.read()\n",
    "\n",
    "# Remove <endofpassage> tags\n",
    "cleaned_corpus = corpus.replace('<endofpassage>', '')\n",
    "\n",
    "# Split by Title\n",
    "entries = re.split(r'Title:', corpus)\n",
    "data = []\n",
    "for entry in entries:\n",
    "    if not entry.strip():\n",
    "        continue\n",
    "    # Split title and passage, keeping 'Passage:' and '<endofpassage>'\n",
    "    title_match = re.match(r'([^\\n]+)\\n(Passage:.*?)(?=Title:|$)', entry, re.DOTALL)\n",
    "    if title_match:\n",
    "        title = title_match.group(1).strip()\n",
    "        passage = title_match.group(2).strip()\n",
    "        data.append([title, passage])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e21b2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of titles: 609\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_name</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200+ of the best deals from Amazon's Cyber Mon...</td>\n",
       "      <td>Table of Contents Table of Contents Echo, Fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASX set to drop as Wall Street’s September slu...</td>\n",
       "      <td>ETF provider Betashares, which manages $30 bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon sellers sound off on the FTC's 'long-ov...</td>\n",
       "      <td>A worker sorts out parcels in the outbound doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christmas Day preview: 49ers, Ravens square of...</td>\n",
       "      <td>Christmas Day isn't just for the NBA, as the N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Raiders vs. Lions live score, updates, highlig...</td>\n",
       "      <td>The Lions just needed to get themselves back i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title_name  \\\n",
       "0  200+ of the best deals from Amazon's Cyber Mon...   \n",
       "1  ASX set to drop as Wall Street’s September slu...   \n",
       "2  Amazon sellers sound off on the FTC's 'long-ov...   \n",
       "3  Christmas Day preview: 49ers, Ravens square of...   \n",
       "4  Raiders vs. Lions live score, updates, highlig...   \n",
       "\n",
       "                                             content  \n",
       "0  Table of Contents Table of Contents Echo, Fire...  \n",
       "1  ETF provider Betashares, which manages $30 bil...  \n",
       "2  A worker sorts out parcels in the outbound doc...  \n",
       "3  Christmas Day isn't just for the NBA, as the N...  \n",
       "4  The Lions just needed to get themselves back i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=['title_name', 'content'])\n",
    "def extract_passages(corpus_text):\n",
    "    \"\"\"\n",
    "    Extracts all passages (including 'Passage:' and '<endofpassage>') from the corpus text.\n",
    "    Returns a list of passage strings.\n",
    "    \"\"\"\n",
    "    pattern = r'Passage:(.*?<endofpassage>)'\n",
    "    passages = re.findall(pattern, corpus_text, re.DOTALL)\n",
    "    # Add back the 'Passage:' prefix to each passage\n",
    "    passages = [p.replace('<endofpassage>', '').strip() for p in passages]\n",
    "    combined_passage = \" \".join(passages)\n",
    "    return combined_passage\n",
    "\n",
    "# Apply to the content of the DataFrame\n",
    "df['content'] = df['content'].apply(extract_passages)\n",
    "print(\"Number of titles:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d6386",
   "metadata": {},
   "source": [
    "**Load Multi-Hop RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c473de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples to query: 2556\n",
      "                                               query              answer  \\\n",
      "0  Who is the individual associated with the cryp...   Sam Bankman-Fried   \n",
      "1  Which individual is implicated in both inflati...        Donald Trump   \n",
      "2  Who is the figure associated with generative A...          Sam Altman   \n",
      "3  Do the TechCrunch article on software companie...                 Yes   \n",
      "4  Which online betting platform provides a welco...  Caesars Sportsbook   \n",
      "\n",
      "                                       evidence_list  \n",
      "0  [{'title': 'The FTX trial is bigger than Sam B...  \n",
      "1  [{'title': 'Donald Trump defrauded banks with ...  \n",
      "2  [{'title': 'OpenAI's ex-chairman accuses board...  \n",
      "3  [{'title': 'Here’s how Rainforest, a budding S...  \n",
      "4  [{'title': '2023 Kentucky online sports bettin...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"VDT2025_Multihop_RAG/MultiHopRAG.json\", \"r\") as f:\n",
    "    query_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "query_df = pd.DataFrame(query_data)\n",
    "query_df = query_df.drop('question_type', axis=1)\n",
    "print(\"Number of samples to query:\", len(query_df))\n",
    "print(query_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e057f7d",
   "metadata": {},
   "source": [
    "## Step 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4d4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "loader = DataFrameLoader(df, page_content_column=\"content\")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "documents = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=documents, \n",
    "                                    embedding=OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "380f2da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a5394",
   "metadata": {},
   "source": [
    "## Step 3: Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65f32bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a103e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c4a60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### START EXAMPLE 5 ###\n",
      "Question: Which online betting platform provides a welcome bonus of up to $1000 in bonus bets for\n",
      "new customers' first losses, runs NBA betting promotions, and is anticipated to extend the same\n",
      "sign-up offer to new users in Vermont, as reported by both CBSSports.com and Sporting News?\n",
      "\n",
      "DECOMPOSITION:\n",
      "1. What online betting platforms offer a welcome bonus of up to $1000 in bonus bets for new\n",
      "customers' first losses?\n",
      "----------------------------------------\n",
      "2. Which online betting platforms run NBA betting promotions?\n",
      "----------------------------------------\n",
      "3. Are there any online betting platforms expected to extend the same sign-up offer to new users in\n",
      "Vermont, as reported by CBSSports.com and Sporting News?\n",
      "----------------------------------------\n",
      "### END EXAMPLE 5 ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = 4\n",
    "end = 5\n",
    "pred_responses = []\n",
    "\n",
    "for idx in range(start, end):\n",
    "    # Run\n",
    "    question = query_df['query'][idx]\n",
    "    questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "    \n",
    "    import textwrap\n",
    "\n",
    "    # For a single long string\n",
    "    print(f\"### START EXAMPLE {idx + 1} ###\")\n",
    "    print(textwrap.fill(f\"Question: {question}\", width=100))\n",
    "    print()\n",
    "    print(\"DECOMPOSITION:\")\n",
    "\n",
    "    metadata = {\n",
    "        \"sub_question\": [],\n",
    "        \"retrieved_docs\": []\n",
    "    }\n",
    "    # For a list of strings\n",
    "    for q in questions:\n",
    "        metadata[\"sub_question\"].append(q)\n",
    "        print(textwrap.fill(q, width=100))\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Answer each sub-question individually \n",
    "\n",
    "    from langchain import hub\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    # RAG prompt\n",
    "    prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "    total_retrieved_docs = []\n",
    "    def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "        \"\"\"RAG on each sub-question\"\"\"\n",
    "        \n",
    "        # Use our decomposition / \n",
    "        sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "        \n",
    "        for sub_question in sub_questions:\n",
    "            \n",
    "            # Retrieve documents for each sub-question\n",
    "            retrieved_docs = retriever.invoke(sub_question)\n",
    "            sub_docs = []\n",
    "            for doc in retrieved_docs:\n",
    "                sub_docs.append(doc)\n",
    "                total_retrieved_docs.append(doc)\n",
    "            metadata[\"retrieved_docs\"].append(sub_docs)        \n",
    "        return sub_questions\n",
    "\n",
    "    # Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "    questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)\n",
    "\n",
    "    print(f\"### END EXAMPLE {idx + 1} ###\")\n",
    "    print()\n",
    "\n",
    "    pred_response = {\n",
    "        \"query\": question,\n",
    "        \"metadata\": metadata,\n",
    "        \"evidence_list\": total_retrieved_docs # a list of Document\n",
    "    }\n",
    "\n",
    "    pred_responses.append(pred_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aeafd",
   "metadata": {},
   "source": [
    "## Step 4: Building Knowledge Graph from Retrieved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db86fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62f9ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Neo4jGraph(refresh_schema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0765a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0  Which online betting platform provides a welco...   \n",
      "\n",
      "                                            metadata  \\\n",
      "0  {'sub_question': ['1. What online betting plat...   \n",
      "\n",
      "                                       evidence_list  \n",
      "0  [page_content='Caesars covers wide range of ma...  \n",
      "Number of evidence documents: 9\n"
     ]
    }
   ],
   "source": [
    "responses_df = pd.DataFrame(pred_responses)\n",
    "print(responses_df.head())\n",
    "print(\"Number of evidence documents:\", len(responses_df['evidence_list'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d91751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_docs = responses_df['evidence_list'][0]\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(extracted_docs)\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "140600fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196a8fc25d6441878660c3e2569acb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# directly show the graph resulting from the given Cypher query\n",
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    #display(widget)\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6739d",
   "metadata": {},
   "source": [
    "### **Query Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13a592fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5e6bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poppy/miniconda3/envs/Neo4j/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1660: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Retriever\n",
    "\n",
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "\n",
    "# Extract entities from text\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting organization and person entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adf96db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10858247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97b04c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b18c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "For Yes/No question, only respond <Yes> or <No>.\n",
    "For question which you think not possible to determine the answer, respond: <Insufficient information.>\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0f9b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-question: What online betting platforms offer a welcome bonus of up to $1000 in bonus bets for new customers' first losses?\n",
      "Search query: What online betting platforms offer a welcome bonus of up to $1000 in bonus bets for new customers' first losses?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Caesars Sportsbook offers a welcome bonus of up to $1,000 in bonus bets for new customers' first losses.\n",
      "----------------------------------------\n",
      "Sub-question: Which online betting platforms run NBA betting promotions?\n",
      "Search query: Which online betting platforms run NBA betting promotions?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: BetMGM and BetRivers run NBA betting promotions.\n",
      "----------------------------------------\n",
      "Sub-question: Are there any online betting platforms expected to extend the same sign-up offer to new users in Vermont, as reported by CBSSports.com and Sporting News?\n",
      "Search query: Are there any online betting platforms expected to extend the same sign-up offer to new users in Vermont, as reported by CBSSports.com and Sporting News?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <Insufficient information.>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sub_questions = responses_df['metadata'][0]['sub_question']\n",
    "\n",
    "chat_history = []\n",
    "for sub_question in sub_questions:\n",
    "    sub_question = re.sub(r'^\\d+\\.\\s*', '', sub_question)\n",
    "    print(f\"Sub-question: {sub_question}\")\n",
    "    response = chain.invoke({\"question\": sub_question, \"chat_history\": []})\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 40)\n",
    "    chat_history.append((sub_question, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "230572ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Which online betting platform is expected to extend the same sign-up offer to new users in Vermont, as reported by CBSSports.com and Sporting News?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 104} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Caesars Sportsbook provides a welcome bonus of up to $1,000 in bonus bets for new customers' first losses, runs NBA betting promotions, and is anticipated to extend the same sign-up offer to new users in Vermont.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "    \"question\": responses_df['query'][0],\n",
    "    \"chat_history\": chat_history,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neo4j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
